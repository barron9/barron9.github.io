<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="/styles/a11y-code.css">
  <link rel="icon" type="image/png" sizes="96x96" href="https://11ty.dev/img/favicon.png">
  <!-- add your favicons here -->
  <!-- <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png"> -->
  <!-- <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png"> -->
  
  <style>
  	:root{--white:#fafafa;--black:#191919;--gray:#212121;--gray-2:#a6a6a6;--blue:#0070ff;--blue-2:#61dcff;--ff-sans-serif:Roboto,Helvetica,sans-serif;--ff-serif:Garamond,serif;--ff-code:monospace}*{box-sizing:border-box}body{--clr-background:var(--white);--clr-text:var(--black);--clr-text-secondary:var(--gray-2);--clr-link:var(--blue);--ff-current:var(--ff-sans-serif);background:var(--clr-background);color:var(--clr-text);font-family:var(--ff-current);padding:12px;line-height:1.5rem}body.dark{--clr-background:var(--black);--clr-text:var(--white);--clr-link:var(--blue-2)}.main-container{max-width:1200px;margin:auto;display:grid;grid-template-columns:1fr 3fr 1fr;gap:3rem}main{min-height:86vh;padding-top:20px}.side-nav{text-align:end;line-height:2rem}.side-nav .site-title{all:unset}.side-nav .site-title:hover{all:unset;cursor:pointer}.side-nav a{all:unset;padding:8px;cursor:pointer}.side-nav a.active,.side-nav a:hover{background:var(--clr-text);color:var(--clr-background)}.side-nav ul{list-style:none}.side-nav ul li{margin:.5rem 0}.side-nav-inner{padding:20px;position:sticky;top:0}a{color:var(--clr-link)}.breadcrumbs{margin-bottom:1rem}article h1{font-size:2rem;line-height:2.5rem}article h1>a,article h2>a,article h3>a{color:var(--clr-text);text-decoration:none}article a:hover{color:var(--clr-link)}article img{max-width:100%}.thumbnail-container img{max-width:100%}.post-entries{padding:0;list-style:none}.post-entry{margin-bottom:4rem}.post-entry__title a{color:var(--clr-text)}.post-entry__title a:hover{color:var(--clr-link)}.post-entry__date{color:var(--clr-text-secondary)}.bibliography ul{padding:0;list-style:none}.bibliography li{padding-left:3rem;text-indent:-3rem;text-align:left;margin:1rem 0 .2rem 0}.main-footer{text-align:center;margin-top:4rem}.post-author{display:none}code{font-size:1.1em}pre{font-size:1em;overflow-x:auto;font-family:var(--ff-code);padding:12px;display:grid}pre code{overflow-x:auto}select{padding:2px;border:1px solid var(--clr-text);background-color:var(--clr-background);color:var(--clr-text);font-size:1em}button{padding:4px;border:1px solid var(--clr-text);color:var(--clr-text);background-color:transparent;cursor:pointer}.side-nav .controls{margin-top:2rem}@media (max-width:1200px){.main-container{grid-template-columns:1fr 3fr}}@media (max-width:720px){.main-container{grid-template-columns:1fr}}@media print{body.dark{--clr-background:#fff;--clr-text:#000;--clr-link:var(--blue)}.side-nav{display:none}.main-footer{display:none}.post-author{display:block}}
  </style>
  <meta property="og:title" content="Actor-Critic: One-Step Update for Actor and Critic Networks">
  <meta property="og:image" content="">
  <meta property="og:description" content="In **Actor-Critic** reinforcement learning methods, there are two main components: the **Actor** (policy network) and the **Critic** (value network). These two networks work together to improve the agent&#39;s performance. Let&#39;s break down the **one-step update** for both networks.">
  <meta name="description" content="In **Actor-Critic** reinforcement learning methods, there are two main components: the **Actor** (policy network) and the **Critic** (value network). These two networks work together to improve the agent&#39;s performance. Let&#39;s break down the **one-step update** for both networks.">
  <meta name="theme-color" content="#0f0f11">
  <title>Actor-Critic: One-Step Update for Actor and Critic Networks | 0v3r.com</title>
</head>

  <body>
    <div class="main-container">
      <aside class="side-nav">
        <div class="side-nav-inner">
          <a class="site-title" href="/">
            0v3r.com
          </a>
          <nav>
  <ul><li>
        <a
          class="nav-link
            
              
            
          "
          href="/"
        > About</a>
      </li><li>
        <a
          class="nav-link
            
               active 
            
          "
          href="/posts"
        > Posts</a>
      </li><li>
        <a
          class="nav-link
            
              
            
          "
          href="/feed.xml"
        > RSS</a>
      </li></ul>
</nav>

          <div class="controls">
            <details>
              <summary>Controls</summary>
              <div>
  <label for="theme">Theme</label>
  <select name="theme" id="theme">
    <option value="system">system</option>
    <option value="light">light</option>
    <option value="dark">dark</option>
  </select>
</div>

              <div>
  <label for="font">Font</label>
  <select name="font" id="font">
    <option value="sansSerif">sans-serif</option>
    <option value="serif">serif</option>
  </select>
</div>

              <div>
                <button onclick="print()">Print this page</button>
              </div>
            </details>
          </div>
        </div>
      </aside>
      <main>
        


<nav class="breadcrumbs">
  
    <span><a href="/posts/">Posts</a></span>
    
      &gt;
    
  
    <span><a href="/posts/post-1/">ac_methods</a></span>
    
  
</nav>


        

<article>
    <h1>Actor-Critic: One-Step Update for Actor and Critic Networks</h1>
    <div class="post-author">
      by
      <strong>
        
          chat.,
        
      </strong>
    <span>Jan 2, 2025</span>
    </div>
    <p><div class="table-of-contents"><h2>Table of Contents</h2><ul><li><a href="#actor-critic%3A-one-step-update-for-actor-and-critic-networks">Actor-Critic: One-Step Update for Actor and Critic Networks</a><ul><li><a href="#1.-actor-network---one-step-update">1. Actor Network - One-Step Update</a></li><li><a href="#2.-critic-network---one-step-update">2. Critic Network - One-Step Update</a></li><li><a href="#summary-of-one-step-updates%3A">Summary of One-Step Updates:</a></li><li><a href="#example-of-one-step-update%3A">Example of One-Step Update:</a></li><li><a href="#key-points%3A">Key Points:</a></li></ul></li></ul></div></p>
<h1 id="actor-critic%3A-one-step-update-for-actor-and-critic-networks" tabindex="-1"><a class="header-anchor" href="#actor-critic%3A-one-step-update-for-actor-and-critic-networks">Actor-Critic: One-Step Update for Actor and Critic Networks</a></h1>
<p>In <strong>Actor-Critic</strong> reinforcement learning methods, there are two main components: the <strong>Actor</strong> (policy network) and the <strong>Critic</strong> (value network). These two networks work together to improve the agent’s performance. Let’s break down the <strong>one-step update</strong> for both networks.</p>
<hr>
<h2 id="1.-actor-network---one-step-update" tabindex="-1"><a class="header-anchor" href="#1.-actor-network---one-step-update">1. Actor Network - One-Step Update</a></h2>
<p>The <strong>Actor</strong> network is responsible for determining which action to take based on the current state. It generates a <strong>policy</strong> which is typically represented as a probability distribution over actions in a given state.</p>
<h3 id="one-step-update-for-actor%3A" tabindex="-1"><a class="header-anchor" href="#one-step-update-for-actor%3A">One-Step Update for Actor:</a></h3>
<p>The actor adjusts its policy based on feedback from the <strong>Critic</strong>. This feedback is provided by the <strong>Temporal Difference (TD) error</strong>, which is calculated by the critic. The TD error reflects how well the actor performed in a given state-action pair.</p>
<h4 id="td-error-calculation%3A" tabindex="-1"><a class="header-anchor" href="#td-error-calculation%3A"><strong>TD Error Calculation</strong>:</a></h4>
<p>The TD error ( \delta_t ) is calculated as follows:</p>
<p>[
\delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)
]</p>
<p>Where:</p>
<ul>
<li>( r_{t+1} ) is the reward received after taking action ( a_t ),</li>
<li>( V(s_t) ) is the value estimate of the current state ( s_t ),</li>
<li>( \gamma ) is the discount factor,</li>
<li>( V(s_{t+1}) ) is the value estimate of the next state ( s_{t+1} ).</li>
</ul>
<h4 id="policy-gradient-update%3A" tabindex="-1"><a class="header-anchor" href="#policy-gradient-update%3A"><strong>Policy Gradient Update</strong>:</a></h4>
<p>The actor updates its policy by adjusting the policy parameters ( \theta_{\text{actor}} ). This update is based on the <strong>TD error</strong>:</p>
<p>[
\theta_{\text{actor}} \leftarrow \theta_{\text{actor}} + \alpha \delta_t \nabla_\theta \log \pi_{\theta}(a_t | s_t)
]</p>
<p>Where:</p>
<ul>
<li>( \alpha ) is the learning rate,</li>
<li>( \delta_t ) is the TD error (from the critic),</li>
<li>( \nabla_\theta \log \pi_{\theta}(a_t | s_t) ) is the gradient of the log-probability of the action taken under the policy.</li>
</ul>
<p>This update aims to:</p>
<ul>
<li><strong>Increase the probability</strong> of actions that lead to higher-than-expected rewards (positive ( \delta_t )),</li>
<li><strong>Decrease the probability</strong> of actions that lead to lower-than-expected rewards (negative ( \delta_t )).</li>
</ul>
<hr>
<h2 id="2.-critic-network---one-step-update" tabindex="-1"><a class="header-anchor" href="#2.-critic-network---one-step-update">2. Critic Network - One-Step Update</a></h2>
<p>The <strong>Critic</strong> network estimates the <strong>value function</strong> (either state-value ( V(s_t) ) or action-value ( Q(s_t, a_t) )) to evaluate the action taken by the actor. The critic’s job is to provide feedback on how good the action taken by the actor was.</p>
<h3 id="one-step-update-for-critic%3A" tabindex="-1"><a class="header-anchor" href="#one-step-update-for-critic%3A">One-Step Update for Critic:</a></h3>
<p>The critic evaluates the accuracy of the actor’s decision by calculating the <strong>TD error</strong>. This error reflects how much better or worse the action was, based on the observed reward and the value of the next state.</p>
<p>For <strong>state-value function</strong> ( V(s) ), the TD error is:</p>
<p>[
\delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)
]</p>
<p>For <strong>action-value function</strong> ( Q(s, a) ), the TD error would be:</p>
<p>[
\delta_t = r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)
]</p>
<h4 id="critic-update-rule%3A" tabindex="-1"><a class="header-anchor" href="#critic-update-rule%3A"><strong>Critic Update Rule</strong>:</a></h4>
<ul>
<li>For <strong>state-value function</strong> ( V(s) ), the update rule for the critic is:</li>
</ul>
<p>[
V(s_t) \leftarrow V(s_t) + \beta \delta_t \nabla_\theta V(s_t)
]</p>
<ul>
<li>For <strong>action-value function</strong> ( Q(s, a) ), the update rule is:</li>
</ul>
<p>[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \beta \delta_t \nabla_\theta Q(s_t, a_t)
]</p>
<p>Where:</p>
<ul>
<li>( \beta ) is the learning rate for the critic,</li>
<li>( \delta_t ) is the TD error (from the actor’s feedback),</li>
<li>( \nabla_\theta V(s_t) ) or ( \nabla_\theta Q(s_t, a_t) ) is the gradient of the value function with respect to the parameters.</li>
</ul>
<hr>
<h2 id="summary-of-one-step-updates%3A" tabindex="-1"><a class="header-anchor" href="#summary-of-one-step-updates%3A">Summary of One-Step Updates:</a></h2>
<table>
<thead>
<tr>
<th><strong>Component</strong></th>
<th><strong>One-Step Update</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Actor</strong></td>
<td>The actor adjusts its policy parameters ( \theta_{\text{actor}} ) using <strong>policy gradients</strong> based on the TD error ( \delta_t ) provided by the critic. The update aims to increase the probability of good actions and decrease the probability of bad actions.</td>
</tr>
<tr>
<td><strong>Critic</strong></td>
<td>The critic updates its value function (either ( V(s) ) or ( Q(s, a) )) using the TD error ( \delta_t ). This feedback helps the critic improve its value estimates over time.</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="example-of-one-step-update%3A" tabindex="-1"><a class="header-anchor" href="#example-of-one-step-update%3A">Example of One-Step Update:</a></h2>
<p>Consider an agent navigating an environment:</p>
<ol>
<li><strong>Actor</strong> selects action ( a_t ) in state ( s_t ).</li>
<li>The environment provides a reward ( r_{t+1} ) and the next state ( s_{t+1} ).</li>
<li>The <strong>Critic</strong> calculates the TD error:</li>
</ol>
<p>[
\delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)
]</p>
<ol start="4">
<li>The <strong>Actor</strong> updates its policy using the TD error. If ( \delta_t &gt; 0 ), the policy is adjusted to favor the action taken. If ( \delta_t &lt; 0 ), the policy is adjusted to avoid that action.</li>
<li>The <strong>Critic</strong> updates its value estimate of the state ( s_t ) (or ( s_t, a_t )) to minimize the TD error.</li>
</ol>
<hr>
<h2 id="key-points%3A" tabindex="-1"><a class="header-anchor" href="#key-points%3A">Key Points:</a></h2>
<ul>
<li>
<p><strong>Actor Network</strong>:</p>
<ul>
<li>Uses the <strong>TD error</strong> to update the policy.</li>
<li>Adjusts the action probabilities to improve reward-maximizing behavior.</li>
</ul>
</li>
<li>
<p><strong>Critic Network</strong>:</p>
<ul>
<li>Uses the <strong>TD error</strong> to update the value function.</li>
<li>Aims to improve the accuracy of value predictions, helping the actor learn from better feedback.</li>
</ul>
</li>
</ul>
<p>The <strong>Actor-Critic</strong> method involves simultaneous updates to both networks: the <strong>actor</strong> learns to take better actions based on the critic’s value function, and the <strong>critic</strong> improves its value estimates based on the actions taken by the actor.</p>

</article>

      </main>
    </div>
    <footer class="main-footer">
  &copy; b 2025
</footer>

  </body>
  
  <script>
  function initThemeHandler(){const e=document.querySelector('select[id="theme"]'),t=window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light",o=localStorage.getItem("theme"),n=document.querySelector('meta[name="theme-color"]'),a=getComputedStyle(document.body);function r(e){"dark"===e?document.body.classList.add("dark"):document.body.classList.remove("dark");const t=a.getPropertyValue("--clr-background");n.setAttribute("content",t)}e.value=o||"system",r("dark"===o||"light"===o?o:t),e.addEventListener("change",(()=>{"system"===e.value?r(t):r(e.value),localStorage.setItem("theme",e.value)}))}function initFontHandler(){const e=getComputedStyle(document.body),t=e.getPropertyValue("--ff-sans-serif"),o=e.getPropertyValue("--ff-serif"),n=localStorage.getItem("font"),a=document.querySelector('select[id="font"]');function r(e){document.body.style.fontFamily="serif"===e?o:t}"serif"===n&&(r(n),a.value=n),a.addEventListener("change",(()=>{r(a.value),localStorage.setItem("font",a.value)}))}initThemeHandler(),initFontHandler();
  </script>
</html>
