<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="/styles/a11y-code.css">
  <link rel="icon" type="image/png" sizes="96x96" href="https://11ty.dev/img/favicon.png">
  <!-- add your favicons here -->
  <!-- <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png"> -->
  <!-- <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png"> -->
  
  <style>
  	:root{--white:#fafafa;--black:#191919;--gray:#212121;--gray-2:#a6a6a6;--blue:#0070ff;--blue-2:#61dcff;--ff-sans-serif:Roboto,Helvetica,sans-serif;--ff-serif:Garamond,serif;--ff-code:monospace}*{box-sizing:border-box}body{--clr-background:var(--white);--clr-text:var(--black);--clr-text-secondary:var(--gray-2);--clr-link:var(--blue);--ff-current:var(--ff-sans-serif);background:var(--clr-background);color:var(--clr-text);font-family:var(--ff-current);padding:12px;line-height:1.5rem}body.dark{--clr-background:var(--black);--clr-text:var(--white);--clr-link:var(--blue-2)}.main-container{max-width:1200px;margin:auto;display:grid;grid-template-columns:1fr 3fr 1fr;gap:3rem}main{min-height:86vh;padding-top:20px}.side-nav{text-align:end;line-height:2rem}.side-nav .site-title{all:unset}.side-nav .site-title:hover{all:unset;cursor:pointer}.side-nav a{all:unset;padding:8px;cursor:pointer}.side-nav a.active,.side-nav a:hover{background:var(--clr-text);color:var(--clr-background)}.side-nav ul{list-style:none}.side-nav ul li{margin:.5rem 0}.side-nav-inner{padding:20px;position:sticky;top:0}a{color:var(--clr-link)}.breadcrumbs{margin-bottom:1rem}article h1{font-size:2rem;line-height:2.5rem}article h1>a,article h2>a,article h3>a{color:var(--clr-text);text-decoration:none}article a:hover{color:var(--clr-link)}article img{max-width:100%}.thumbnail-container img{max-width:100%}.post-entries{padding:0;list-style:none}.post-entry{margin-bottom:4rem}.post-entry__title a{color:var(--clr-text)}.post-entry__title a:hover{color:var(--clr-link)}.post-entry__date{color:var(--clr-text-secondary)}.bibliography ul{padding:0;list-style:none}.bibliography li{padding-left:3rem;text-indent:-3rem;text-align:left;margin:1rem 0 .2rem 0}.main-footer{text-align:center;margin-top:4rem}.post-author{display:none}code{font-size:1.1em}pre{font-size:1em;overflow-x:auto;font-family:var(--ff-code);padding:12px;display:grid}pre code{overflow-x:auto}select{padding:2px;border:1px solid var(--clr-text);background-color:var(--clr-background);color:var(--clr-text);font-size:1em}button{padding:4px;border:1px solid var(--clr-text);color:var(--clr-text);background-color:transparent;cursor:pointer}.side-nav .controls{margin-top:2rem}@media (max-width:1200px){.main-container{grid-template-columns:1fr 3fr}}@media (max-width:720px){.main-container{grid-template-columns:1fr}}@media print{body.dark{--clr-background:#fff;--clr-text:#000;--clr-link:var(--blue)}.side-nav{display:none}.main-footer{display:none}.post-author{display:block}}
  </style>
  <meta property="og:title" content="Actor-Critic: One-Step Update for Actor and Critic Networks">
  <meta property="og:image" content="">
  <meta property="og:description" content="In **Actor-Critic** reinforcement learning methods, there are two main components: the **Actor** (policy network) and the **Critic** (value network). These two networks work together to improve the agent&#39;s performance. Let&#39;s break down the **one-step update** for both networks.">
  <meta name="description" content="In **Actor-Critic** reinforcement learning methods, there are two main components: the **Actor** (policy network) and the **Critic** (value network). These two networks work together to improve the agent&#39;s performance. Let&#39;s break down the **one-step update** for both networks.">
  <meta name="theme-color" content="#0f0f11">
  <title>Actor-Critic: One-Step Update for Actor and Critic Networks | 0v3r.com</title>
</head>

  <body>
    <div class="main-container">
      <aside class="side-nav">
        <div class="side-nav-inner">
          <a class="site-title" href="/">
            0v3r.com
          </a>
          <nav>
  <ul><li>
        <a
          class="nav-link
            
              
            
          "
          href="/"
        > About</a>
      </li><li>
        <a
          class="nav-link
            
               active 
            
          "
          href="/posts"
        > Posts</a>
      </li><li>
        <a
          class="nav-link
            
              
            
          "
          href="/feed.xml"
        > RSS</a>
      </li></ul>
</nav>

          <div class="controls">
            <details>
              <summary>Controls</summary>
              <div>
  <label for="theme">Theme</label>
  <select name="theme" id="theme">
    <option value="system">system</option>
    <option value="light">light</option>
    <option value="dark">dark</option>
  </select>
</div>

              <div>
  <label for="font">Font</label>
  <select name="font" id="font">
    <option value="sansSerif">sans-serif</option>
    <option value="serif">serif</option>
  </select>
</div>

              <div>
                <button onclick="print()">Print this page</button>
              </div>
            </details>
          </div>
        </div>
      </aside>
      <main>
        


<nav class="breadcrumbs">
  
    <span><a href="/posts/">Posts</a></span>
    
      &gt;
    
  
    <span><a href="/posts/post-1/">ac_methods</a></span>
    
  
</nav>


        

<article>
    <h1>Actor-Critic: One-Step Update for Actor and Critic Networks</h1>
    <div class="post-author">
      by
      <strong>
        
          chat.,
        
      </strong>
    <span>Jan 2, 2025</span>
    </div>
    <p><div class="table-of-contents"><h2>Table of Contents</h2><ul><li><a href="#actor-critic%3A-one-step-update-for-actor-and-critic-networks">Actor-Critic: One-Step Update for Actor and Critic Networks</a><ul><li><a href="#1.-actor-network---one-step-update">1. Actor Network - One-Step Update</a></li><li><a href="#2.-critic-network---one-step-update">2. Critic Network - One-Step Update</a></li><li><a href="#summary-of-one-step-updates%3A">Summary of One-Step Updates:</a></li><li><a href="#example-of-one-step-update%3A">Example of One-Step Update:</a></li><li><a href="#key-points%3A">Key Points:</a></li></ul></li></ul></div></p>
<h1 id="actor-critic%3A-one-step-update-for-actor-and-critic-networks" tabindex="-1"><a class="header-anchor" href="#actor-critic%3A-one-step-update-for-actor-and-critic-networks">Actor-Critic: One-Step Update for Actor and Critic Networks</a></h1>
<p>In <strong>Actor-Critic</strong> reinforcement learning methods, there are two main components: the <strong>Actor</strong> (policy network) and the <strong>Critic</strong> (value network). These two networks work together to improve the agent’s performance. Let’s break down the <strong>one-step update</strong> for both networks.</p>
<hr>
<h2 id="1.-actor-network---one-step-update" tabindex="-1"><a class="header-anchor" href="#1.-actor-network---one-step-update">1. Actor Network - One-Step Update</a></h2>
<p>The <strong>Actor</strong> network is responsible for determining which action to take based on the current state. It generates a <strong>policy</strong> which is typically represented as a probability distribution over actions in a given state.</p>
<h3 id="one-step-update-for-actor%3A" tabindex="-1"><a class="header-anchor" href="#one-step-update-for-actor%3A">One-Step Update for Actor:</a></h3>
<p>The actor adjusts its policy based on feedback from the <strong>Critic</strong>. This feedback is provided by the <strong>Temporal Difference (TD) error</strong>, which is calculated by the critic. The TD error reflects how well the actor performed in a given state-action pair.</p>
<h4 id="td-error-calculation%3A" tabindex="-1"><a class="header-anchor" href="#td-error-calculation%3A"><strong>TD Error Calculation</strong>:</a></h4>
<p>The TD error $ \delta_t $ is calculated as follows:</p>
<p><mjx-container class="MathJax" jax="SVG" style="direction: ltr; position: relative;"><svg style="overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="28.074ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 12408.9 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D6FF" d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(477,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(1060,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(2115.8,0)"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(484,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(361,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mn" transform="translate(1139,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="mo" transform="translate(4031,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(5031.2,0)"><path data-c="1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(5574.2,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(6343.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(6732.2,0)"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width: 3;"/></g><g data-mml-node="TeXAtom" transform="translate(502,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(361,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mn" transform="translate(1139,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width: 3;"/></g></g></g><g data-mml-node="mo" transform="translate(8443.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(9054.4,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(10054.6,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z" style="stroke-width: 3;"/></g><g data-mml-node="mo" transform="translate(10823.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width: 3;"/></g><g data-mml-node="msub" transform="translate(11212.6,0)"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width: 3;"/></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width: 3;"/></g></g><g data-mml-node="mo" transform="translate(12019.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width: 3;"/></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>δ</mi><mi>t</mi></msub><mo>=</mo><msub><mi>r</mi><mrow data-mjx-texclass="ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><mi>V</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mrow data-mjx-texclass="ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>−</mo><mi>V</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></p>
<p>Where:</p>
<ul>
<li>$ r_{t+1} $ is the reward received after taking action $ a_t $,</li>
<li>$ V(s_t) $ is the value estimate of the current state $ s_t $,</li>
<li>$ \gamma $ is the discount factor,</li>
<li>$ V(s_{t+1}) $ is the value estimate of the next state $ s_{t+1} $.</li>
</ul>
<h4 id="policy-gradient-update%3A" tabindex="-1"><a class="header-anchor" href="#policy-gradient-update%3A"><strong>Policy Gradient Update</strong>:</a></h4>
<p>The actor updates its policy by adjusting the policy parameters $ \theta_{\text{actor}} $. This update is based on the <strong>TD error</strong>:</p>
<p>[
\theta_{\text{actor}} \leftarrow \theta_{\text{actor}} + \alpha \delta_t \nabla_\theta \log \pi_{\theta}(a_t | s_t)
]</p>
<p>Where:</p>
<ul>
<li>$ \alpha $ is the learning rate,</li>
<li>$ \delta_t $ is the TD error (from the critic),</li>
<li>$ \nabla_\theta \log \pi_{\theta}(a_t | s_t) $ is the gradient of the log-probability of the action taken under the policy.</li>
</ul>
<p>This update aims to:</p>
<ul>
<li><strong>Increase the probability</strong> of actions that lead to higher-than-expected rewards (positive $ \delta_t $),</li>
<li><strong>Decrease the probability</strong> of actions that lead to lower-than-expected rewards (negative $ \delta_t $).</li>
</ul>
<hr>
<h2 id="2.-critic-network---one-step-update" tabindex="-1"><a class="header-anchor" href="#2.-critic-network---one-step-update">2. Critic Network - One-Step Update</a></h2>
<p>The <strong>Critic</strong> network estimates the <strong>value function</strong> (either state-value $ V(s_t) $ or action-value $ Q(s_t, a_t) $) to evaluate the action taken by the actor. The critic’s job is to provide feedback on how good the action taken by the actor was.</p>
<h3 id="one-step-update-for-critic%3A" tabindex="-1"><a class="header-anchor" href="#one-step-update-for-critic%3A">One-Step Update for Critic:</a></h3>
<p>The critic evaluates the accuracy of the actor’s decision by calculating the <strong>TD error</strong>. This error reflects how much better or worse the action was, based on the observed reward and the value of the next state.</p>
<p>For <strong>state-value function</strong> $ V(s) $, the TD error is:</p>
<p>[
\delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)
]</p>
<p>For <strong>action-value function</strong> $ Q(s, a) $, the TD error would be:</p>
<p>[
\delta_t = r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)
]</p>
<h4 id="critic-update-rule%3A" tabindex="-1"><a class="header-anchor" href="#critic-update-rule%3A"><strong>Critic Update Rule</strong>:</a></h4>
<ul>
<li>For <strong>state-value function</strong> $ V(s) $, the update rule for the critic is:</li>
</ul>
<p>[
V(s_t) \leftarrow V(s_t) + \beta \delta_t \nabla_\theta V(s_t)
]</p>
<ul>
<li>For <strong>action-value function</strong> $ Q(s, a) $, the update rule is:</li>
</ul>
<p>[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \beta \delta_t \nabla_\theta Q(s_t, a_t)
]</p>
<p>Where:</p>
<ul>
<li>$ \beta $ is the learning rate for the critic,</li>
<li>$ \delta_t $ is the TD error (from the actor’s feedback),</li>
<li>$ \nabla_\theta V(s_t) $ or $ \nabla_\theta Q(s_t, a_t) $ is the gradient of the value function with respect to the parameters.</li>
</ul>
<hr>
<h2 id="summary-of-one-step-updates%3A" tabindex="-1"><a class="header-anchor" href="#summary-of-one-step-updates%3A">Summary of One-Step Updates:</a></h2>
<table>
<thead>
<tr>
<th><strong>Component</strong></th>
<th><strong>One-Step Update</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Actor</strong></td>
<td>The actor adjusts its policy parameters $ \theta_{\text{actor}} $ using <strong>policy gradients</strong> based on the TD error $ \delta_t $ provided by the critic. The update aims to increase the probability of good actions and decrease the probability of bad actions.</td>
</tr>
<tr>
<td><strong>Critic</strong></td>
<td>The critic updates its value function (either $ V(s) $ or $ Q(s, a) $) using the TD error $ \delta_t $. This feedback helps the critic improve its value estimates over time.</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="example-of-one-step-update%3A" tabindex="-1"><a class="header-anchor" href="#example-of-one-step-update%3A">Example of One-Step Update:</a></h2>
<p>Consider an agent navigating an environment:</p>
<ol>
<li><strong>Actor</strong> selects action $ a_t $ in state $ s_t $.</li>
<li>The environment provides a reward $ r_{t+1} $ and the next state $ s_{t+1} $.</li>
<li>The <strong>Critic</strong> calculates the TD error:</li>
</ol>
<p>[
\delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)
]</p>
<ol start="4">
<li>The <strong>Actor</strong> updates its policy using the TD error. If $ \delta_t &gt; 0 $, the policy is adjusted to favor the action taken. If $ \delta_t &lt; 0 $, the policy is adjusted to avoid that action.</li>
<li>The <strong>Critic</strong> updates its value estimate of the state $ s_t $ (or $ s_t, a_t $) to minimize the TD error.</li>
</ol>
<hr>
<h2 id="key-points%3A" tabindex="-1"><a class="header-anchor" href="#key-points%3A">Key Points:</a></h2>
<ul>
<li>
<p><strong>Actor Network</strong>:</p>
<ul>
<li>Uses the <strong>TD error</strong> to update the policy.</li>
<li>Adjusts the action probabilities to improve reward-maximizing behavior.</li>
</ul>
</li>
<li>
<p><strong>Critic Network</strong>:</p>
<ul>
<li>Uses the <strong>TD error</strong> to update the value function.</li>
<li>Aims to improve the accuracy of value predictions, helping the actor learn from better feedback.</li>
</ul>
</li>
</ul>
<p>The <strong>Actor-Critic</strong> method involves simultaneous updates to both networks: the <strong>actor</strong> learns to take better actions based on the critic’s value function, and the <strong>critic</strong> improves its value estimates based on the actions taken by the actor.</p>

</article>

      </main>
    </div>
    <footer class="main-footer">
  &copy; b 2025
</footer>

  </body>
  
  <script>
  function initThemeHandler(){const e=document.querySelector('select[id="theme"]'),t=window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light",o=localStorage.getItem("theme"),n=document.querySelector('meta[name="theme-color"]'),a=getComputedStyle(document.body);function r(e){"dark"===e?document.body.classList.add("dark"):document.body.classList.remove("dark");const t=a.getPropertyValue("--clr-background");n.setAttribute("content",t)}e.value=o||"system",r("dark"===o||"light"===o?o:t),e.addEventListener("change",(()=>{"system"===e.value?r(t):r(e.value),localStorage.setItem("theme",e.value)}))}function initFontHandler(){const e=getComputedStyle(document.body),t=e.getPropertyValue("--ff-sans-serif"),o=e.getPropertyValue("--ff-serif"),n=localStorage.getItem("font"),a=document.querySelector('select[id="font"]');function r(e){document.body.style.fontFamily="serif"===e?o:t}"serif"===n&&(r(n),a.value=n),a.addEventListener("change",(()=>{r(a.value),localStorage.setItem("font",a.value)}))}initThemeHandler(),initFontHandler();
  </script>
</html>
